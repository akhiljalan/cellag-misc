{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.size'] = 20.0\n",
    "plt.rcParams['xtick.labelsize'] = 20.0\n",
    "plt.rcParams['ytick.labelsize'] = 20.0\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_learning_utils as gl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of this notebook\n",
    "\n",
    "* Implement faster, sparse-matrix based ways of generate Boolean sampling data\n",
    "\n",
    "* Implement a simple occlusion method that zeroes out rows/columns of a matrix\n",
    "\n",
    "* Test the solver on occluded versions of human PPI data. Unfortunately, it is simply too slow. \n",
    "\n",
    "We need to either run sparsity based methods, or implement on GPUs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion model \n",
    "\n",
    "We are given a ground truth $A^s$ through an unbiased sampling procedure from the base PPI. \n",
    "\n",
    "We generate some number of P_i by knocking certain edges from P, then sampling from than biased version to get P_i. \n",
    "\n",
    "Run the algorithm, then ask: \n",
    "\n",
    "* How many edges can we recover?\n",
    "\n",
    "Can use the usual binary classification metrics, e.g. precision, recall, F1, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = scipy.sparse.load_npz('data/adj_matrix_sparse_restricted_9606.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11916x11916 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5963604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree =  np.max(sparse_matrix)\n",
    "sparse_matrix /= max_degree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faster bernoulli sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_occluded_p(sparse_M, frac_to_occlude = 0.01): \n",
    "    n = sparse_M.shape[0]\n",
    "    num_to_occlude = int(frac_to_occlude * n)\n",
    "    occluded_indices = np.random.choice(n, size=num_to_occlude, replace=False)\n",
    "    return zero_rows_cols(sparse_M, occluded_indices)\n",
    "\n",
    "def zero_rows_cols(M, row_indices):\n",
    "    '''\n",
    "    Zeroes out the rows/cols in row_indices. \n",
    "    M is a sparse matrix type. \n",
    "    '''\n",
    "    diag = scipy.sparse.eye(M.shape[0]).tolil()\n",
    "    for r in row_indices:\n",
    "        diag[r, r] = 0\n",
    "    return diag.dot(M).dot(diag)\n",
    "\n",
    "def bin_samples_rand2(n, mu, seed=0):\n",
    "    '''\n",
    "    mu: List of floats in [0,1] describing Bernoulli mean probabilities. \n",
    "    n: number of samples per entries of mu. \n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return (rng.random(size=(len(mu), n)) < mu[:, None]).astype(np.uint8)\n",
    "\n",
    "def gen_sparse_sample_boolean_mat(sparse_mat): \n",
    "    nonzero_float_entries = scipy.sparse.triu(sparse_mat, k=1).data\n",
    "    sparse_tri = scipy.sparse.csr_matrix(scipy.sparse.triu(sparse_mat, k = 1))\n",
    "    sample_bool = bin_samples_rand2(1, nonzero_float_entries).squeeze(-1)\n",
    "    sparse_tri.data = sample_bool\n",
    "    symm_sparse = sparse_tri + sparse_tri.T\n",
    "    return symm_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5 \n",
    "occluded_ground_truths = [gen_occluded_p(sparse_matrix) for _ in range(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonzero_float_entries = scipy.sparse.triu(sparse_matrix, k=1).data\n",
    "# sparse_tri = scipy.sparse.csr_matrix(scipy.sparse.triu(sparse_matrix, k = 1))\n",
    "# sample_bool = bin_samples_rand2(1, nonzero_float_entries).squeeze(-1)\n",
    "# sparse_tri.data = sample_bool\n",
    "# symm_sparse = sparse_tri + sparse_tri.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_mat = gen_sparse_sample_boolean_mat(sparse_matrix)\n",
    "sample_mats = [gen_sparse_sample_boolean_mat(occluded_ground_truth) for occluded_ground_truth in occluded_ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rewrite algo for sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_with_params_sparse(eta_arr, validation_mat, sample_mats, delta, num_eigs_included=None, verbose=False):\n",
    "    # if verbose:\n",
    "    #     print(eta_arr)\n",
    "    if num_eigs_included is None:\n",
    "        num_eigs_included = validation_mat.shape[0]\n",
    "    P_hat = gl.matrix_lin_combo_pos_sign(eta_arr, sample_mats, sparse=True)\n",
    "\n",
    "    # singular values, decreasing order\n",
    "    diff_sing_values = scipy.sparse.linalg.svds(validation_mat - P_hat, \n",
    "                                                k=num_eigs_included, return_singular_vectors=False)\n",
    "\n",
    "    # scipy.linalg.svdvals(validation_mat - P_hat)\n",
    "\n",
    "    def ob_fn(k): \n",
    "        return sum(diff_sing_values[:k]) - k * delta\n",
    "\n",
    "    all_obj_values = [ob_fn(k) for k in range(1, num_eigs_included + 1)]\n",
    "    max_obj_index = np.argmax(all_obj_values)\n",
    "\n",
    "    return all_obj_values[max_obj_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_init = gl.generate_random_eta(len(sample_mats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_swap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m delta \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      2\u001b[0m objective \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m eta_arr: objective_with_params_sparse(eta_arr, validation_mat, sample_mats, delta, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m result \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m      5\u001b[0m     objective,\n\u001b[1;32m      6\u001b[0m     eta_init,\n\u001b[1;32m      7\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSLSQP\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     bounds\u001b[39m=\u001b[39;49m[(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(sample_mats))],\n\u001b[1;32m     10\u001b[0m     options\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     11\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m10000\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mdisp\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    703\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    704\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mslsqp\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 705\u001b[0m     res \u001b[39m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[1;32m    706\u001b[0m                           constraints, callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    707\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrust-constr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    708\u001b[0m     res \u001b[39m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[1;32m    709\u001b[0m                                        bounds, constraints,\n\u001b[1;32m    710\u001b[0m                                        callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_slsqp_py.py:374\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    371\u001b[0m     xu[infbnd[:, \u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m    373\u001b[0m \u001b[39m# ScalarFunction provides function and gradient evaluation\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(func, x, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    375\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step,\n\u001b[1;32m    376\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds)\n\u001b[1;32m    377\u001b[0m \u001b[39m# gh11403 SLSQP sometimes exceeds bounds by 1 or 2 ULP, make sure this\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m# doesn't get sent to the func/grad evaluator.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m wrapped_fun \u001b[39m=\u001b[39m _clip_x_for_func(sf\u001b[39m.\u001b[39mfun, new_bounds)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    328\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[1;32m    330\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    333\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(eta_arr)\u001b[0m\n\u001b[1;32m      1\u001b[0m delta \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m objective \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m eta_arr: objective_with_params_sparse(eta_arr, validation_mat, sample_mats, delta, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      4\u001b[0m result \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39moptimize\u001b[39m.\u001b[39mminimize(\n\u001b[1;32m      5\u001b[0m     objective,\n\u001b[1;32m      6\u001b[0m     eta_init,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m )\n",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m, in \u001b[0;36mobjective_with_params_sparse\u001b[0;34m(eta_arr, validation_mat, sample_mats, delta, num_eigs_included, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m num_eigs_included \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     num_eigs_included \u001b[39m=\u001b[39m validation_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m P_hat \u001b[39m=\u001b[39m gl\u001b[39m.\u001b[39;49mmatrix_lin_combo_pos_sign(eta_arr, sample_mats, sparse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# singular values, decreasing order\u001b[39;00m\n\u001b[1;32m      9\u001b[0m diff_sing_values \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msvds(validation_mat \u001b[39m-\u001b[39m P_hat, \n\u001b[1;32m     10\u001b[0m                                             k\u001b[39m=\u001b[39mnum_eigs_included, return_singular_vectors\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/cellag-misc/graph_learning_utils.py:97\u001b[0m, in \u001b[0;36mmatrix_lin_combo_pos_sign\u001b[0;34m(eta_arr, sample_mats, sparse)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatrix_lin_combo_pos_sign\u001b[39m(eta_arr, sample_mats, sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):  \n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m sparse: \n\u001b[0;32m---> 97\u001b[0m         \u001b[39mreturn\u001b[39;00m scipy\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mcsr_matrix\u001b[39m.\u001b[39;49msum([eta \u001b[39m*\u001b[39;49m sample_mat \u001b[39mfor\u001b[39;49;00m eta, sample_mat \n\u001b[1;32m     98\u001b[0m                                             \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(eta_arr, sample_mats)], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msum([eta \u001b[39m*\u001b[39m sample_mat \u001b[39mfor\u001b[39;00m eta, sample_mat \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eta_arr, sample_mats)], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3-akhil/lib/python3.10/site-packages/scipy/sparse/_compressed.py:606\u001b[0m, in \u001b[0;36m_cs_matrix.sum\u001b[0;34m(self, axis, dtype, out)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sum the matrix over the given axis.  If the axis is None, sum\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mover both rows and columns, returning a scalar.\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39m# The spmatrix base class already does axis=0 and axis=1 efficiently\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[39m# so we only do the case axis=None here\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mblocksize\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m         axis \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_swap(((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)))[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    607\u001b[0m     \u001b[39m# faster than multiplication for large minor axis in CSC/CSR\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     res_dtype \u001b[39m=\u001b[39m get_sum_dtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    609\u001b[0m     ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mres_dtype)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_swap'"
     ]
    }
   ],
   "source": [
    "delta = 0.0\n",
    "objective = lambda eta_arr: objective_with_params_sparse(eta_arr, validation_mat, sample_mats, delta, verbose=False)\n",
    "\n",
    "result = scipy.optimize.minimize(\n",
    "    objective,\n",
    "    eta_init,\n",
    "    method='SLSQP',\n",
    "    jac=None,\n",
    "    bounds=[(0, 1) for _ in range(len(sample_mats))],\n",
    "    options={\n",
    "        'maxiter': 10000,\n",
    "        'disp': False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run basic algo, numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_init = gl.generate_random_eta(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mats_np = [sm.toarray() for sm in sample_mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_mat_np = validation_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = gl.run_scipy_minimize(sample_mats_np, validation_mat_np, \n",
    "#                                delta=0.01, eta_init=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def track(f, out_values=None, out_args=None, out_kwargs=None):\n",
    "#     def wrapped(*args, **kwargs):\n",
    "#         if out_args is not None: out_args.append(args)\n",
    "#         if out_kwargs is not None: out_kwargs.append(kwargs)\n",
    "#         val = f(*args, **kwargs)\n",
    "#         if out_values is not None: out_values.append(val)\n",
    "#         return val\n",
    "#     return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars, vals = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbackF(eta_current):\n",
    "    print(eta_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.001\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective = lambda eta_arr: gl.objective_with_params(eta_arr, validation_mat_np, sample_mats_np, delta, verbose=False)\n",
    "# print('begin iter')\n",
    "# # constraints={'type': 'eq', 'fun': simplex_constraint},\n",
    "# result = scipy.optimize.minimize(\n",
    "#     objective,\n",
    "#     gl.generate_random_eta(len(sample_mats)),\n",
    "#     callback=callbackF,\n",
    "#     method='SLSQP',\n",
    "#     jac=None,\n",
    "#     bounds=[(0, 1) for _ in range(len(sample_mats))],\n",
    "#     options={\n",
    "#         'maxiter': 10000,\n",
    "#         'disp': verbose\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: In 4.5 minutes (on the vastness machine) we can't even perform a single iteration of the solver. \n",
    "\n",
    "We will need some tricks to speed this up - in particular, we need to either: \n",
    "\n",
    "* Do a GPU based solver \n",
    "\n",
    "* Use the sparse matrix structure of our data. Our average edge density is something like 4%, so this should be quite doable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Blockmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 \n",
    "n = 200\n",
    "comm_size = int(n / k)\n",
    "p_in = 0.5\n",
    "p_out = 0.1\n",
    "probs_mat = p_out * np.ones((2, 2)) + (p_in - p_out) * np.diag(np.ones(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.1],\n",
       "       [0.1, 0.5]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sbm = nx.stochastic_block_model(sizes=[comm_size for _ in range(k)], \n",
    "                          p=probs_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
